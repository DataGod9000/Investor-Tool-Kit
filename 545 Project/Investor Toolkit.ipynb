{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **5450 Group Project**\n","# **Investor Toolkit: Asset Clustering, Sentiment-Based Trading Bot, and Price Action Model for Short Term Scalping**\n","\n","Davit Barseghyan, Qixiu Quan, Jeff Grant, Eng Wei Jie Joseph\n"],"metadata":{"id":"tBgUfomfq5KE"}},{"cell_type":"markdown","source":["# Introduction\n"],"metadata":{"id":"OUZiH1JAqBcr"}},{"cell_type":"markdown","source":["In this project, we aim to create a series of tools that an investor can use to create weighted portfolio of stocks optimized to maximize returns based on daily performance, predict market movements using sentiment analysis of tweets about various stocks, and analyze market movement to \"scalp\" stocks by predicting cyclic dips and peaks in prices to buy at low prices and sell at high prices.\n","\n","This project is divided into three major parts:\n","\n","\n","1.   Asset Clustering: Grouping S&P 500 stocks based on annualized returns and volatility to form optimized investment portfolios.\n","2.   Sentiment-Based Trading Bot: Building an autonomous trading bot driven by sentiment analysis of stock-related tweets.\n","3.   A Price Action Model for short term scalping by predicting dips in the market.\n","\n","\n","\n","\n"],"metadata":{"id":"nC9arVt3qKaH"}},{"cell_type":"markdown","source":["#**Part 1: Asset Clustering**\n","In this part of our project we use the Yahoo finance API to retrieve historical ajdusted closing price data for stocks in the S&P 500 over the five-year period from 2014 to 2018 (We chose this time frame to avoid uncharacteristic turbulence during and immediately after the Covid-19 pandemic).  We then group stocks based on their annualized returns and volatility.  By analyzing these clusters we can create a portfolio expected, based on historical returns, to offer both diversification based on sector and maximized returns.\n","\n"],"metadata":{"id":"1R4Q9kXptGxL"}},{"cell_type":"markdown","source":["Phase 1: Data Acquisition and Cleaning\n","We retrieved historical adjusted closing price data for stocks in the S&P 500 using the Yahoo Finance API."],"metadata":{"id":"UuQrZQTlDmKt"}},{"cell_type":"markdown","source":["Import required libraries\n","\n"],"metadata":{"id":"Fl1U6cUVvsWK"}},{"cell_type":"code","source":["pip install --upgrade yfinance"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzPSGWyWt-Ha","outputId":"207370ab-b7bf-4792-895e-2c8cf926f04f","executionInfo":{"status":"ok","timestamp":1735721590997,"user_tz":-480,"elapsed":13187,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.50)\n","Collecting yfinance\n","  Downloading yfinance-0.2.51-py2.py3-none-any.whl.metadata (5.5 kB)\n","Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.2.2)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.26.4)\n","Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n","Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n","Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (5.3.0)\n","Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.6)\n","Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.2)\n","Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.6)\n","Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.8)\n","Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n","Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.12.14)\n","Downloading yfinance-0.2.51-py2.py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.7/104.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: yfinance\n","  Attempting uninstall: yfinance\n","    Found existing installation: yfinance 0.2.50\n","    Uninstalling yfinance-0.2.50:\n","      Successfully uninstalled yfinance-0.2.50\n","Successfully installed yfinance-0.2.51\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import yfinance as yf\n","import logging\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score"],"metadata":{"id":"RfEHi76JbvS4","executionInfo":{"status":"ok","timestamp":1735721605286,"user_tz":-480,"elapsed":14292,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Create list of stock tickers for S&P 500 companies by collecting and cleaning data from Wikipedia"],"metadata":{"id":"QaSY4ISRvz5_"}},{"cell_type":"code","source":["sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n","\n","# Read in the url and scrape ticker data\n","data_table = pd.read_html(sp500_url)\n","tickers = data_table[0]['Symbol'].values.tolist()\n","tickers = [s.replace('\\n', '') for s in tickers]\n","tickers = [s.replace('.', '-') for s in tickers]\n","tickers = [s.replace(' ', '') for s in tickers]\n","\n","print(tickers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zE9iTBQ8Fa_p","outputId":"4b7b00b5-139e-46a2-f25d-34b79dc5aed8","executionInfo":{"status":"ok","timestamp":1735721605992,"user_tz":-480,"elapsed":709,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD', 'ABNB', 'AKAM', 'ALB', 'ARE', 'ALGN', 'ALLE', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AMCR', 'AEE', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'AME', 'AMGN', 'APH', 'ADI', 'ANSS', 'AON', 'APA', 'APO', 'AAPL', 'AMAT', 'APTV', 'ACGL', 'ADM', 'ANET', 'AJG', 'AIZ', 'T', 'ATO', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'AXON', 'BKR', 'BALL', 'BAC', 'BAX', 'BDX', 'BRK-B', 'BBY', 'TECH', 'BIIB', 'BLK', 'BX', 'BK', 'BA', 'BKNG', 'BWA', 'BSX', 'BMY', 'AVGO', 'BR', 'BRO', 'BF-B', 'BLDR', 'BG', 'BXP', 'CHRW', 'CDNS', 'CZR', 'CPT', 'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CARR', 'CAT', 'CBOE', 'CBRE', 'CDW', 'CE', 'COR', 'CNC', 'CNP', 'CF', 'CRL', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CAG', 'COP', 'ED', 'STZ', 'CEG', 'COO', 'CPRT', 'GLW', 'CPAY', 'CTVA', 'CSGP', 'COST', 'CTRA', 'CRWD', 'CCI', 'CSX', 'CMI', 'CVS', 'DHR', 'DRI', 'DVA', 'DAY', 'DECK', 'DE', 'DELL', 'DAL', 'DVN', 'DXCM', 'FANG', 'DLR', 'DFS', 'DG', 'DLTR', 'D', 'DPZ', 'DOV', 'DOW', 'DHI', 'DTE', 'DUK', 'DD', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'ELV', 'EMR', 'ENPH', 'ETR', 'EOG', 'EPAM', 'EQT', 'EFX', 'EQIX', 'EQR', 'ERIE', 'ESS', 'EL', 'EG', 'EVRG', 'ES', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FDS', 'FICO', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FI', 'FMC', 'F', 'FTNT', 'FTV', 'FOXA', 'FOX', 'BEN', 'FCX', 'GRMN', 'IT', 'GE', 'GEHC', 'GEV', 'GEN', 'GNRC', 'GD', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GL', 'GDDY', 'GS', 'HAL', 'HIG', 'HAS', 'HCA', 'DOC', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HWM', 'HPQ', 'HUBB', 'HUM', 'HBAN', 'HII', 'IBM', 'IEX', 'IDXX', 'ITW', 'INCY', 'IR', 'PODD', 'INTC', 'ICE', 'IFF', 'IP', 'IPG', 'INTU', 'ISRG', 'IVZ', 'INVH', 'IQV', 'IRM', 'JBHT', 'JBL', 'JKHY', 'J', 'JNJ', 'JCI', 'JPM', 'JNPR', 'K', 'KVUE', 'KDP', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KKR', 'KLAC', 'KHC', 'KR', 'LHX', 'LH', 'LRCX', 'LW', 'LVS', 'LDOS', 'LEN', 'LII', 'LLY', 'LIN', 'LYV', 'LKQ', 'LMT', 'L', 'LOW', 'LULU', 'LYB', 'MTB', 'MPC', 'MKTX', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MTCH', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'META', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MRNA', 'MHK', 'MOH', 'TAP', 'MDLZ', 'MPWR', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'NDAQ', 'NTAP', 'NFLX', 'NEM', 'NWSA', 'NWS', 'NEE', 'NKE', 'NI', 'NDSN', 'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'NVR', 'NXPI', 'ORLY', 'OXY', 'ODFL', 'OMC', 'ON', 'OKE', 'ORCL', 'OTIS', 'PCAR', 'PKG', 'PLTR', 'PANW', 'PARA', 'PH', 'PAYX', 'PAYC', 'PYPL', 'PNR', 'PEP', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PNC', 'POOL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PTC', 'PSA', 'PHM', 'PWR', 'QCOM', 'DGX', 'RL', 'RJF', 'RTX', 'O', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RVTY', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'SPGI', 'CRM', 'SBAC', 'SLB', 'STX', 'SRE', 'NOW', 'SHW', 'SPG', 'SWKS', 'SJM', 'SW', 'SNA', 'SOLV', 'SO', 'LUV', 'SWK', 'SBUX', 'STT', 'STLD', 'STE', 'SYK', 'SMCI', 'SYF', 'SNPS', 'SYY', 'TMUS', 'TROW', 'TTWO', 'TPR', 'TRGP', 'TGT', 'TEL', 'TDY', 'TFX', 'TER', 'TSLA', 'TXN', 'TPL', 'TXT', 'TMO', 'TJX', 'TSCO', 'TT', 'TDG', 'TRV', 'TRMB', 'TFC', 'TYL', 'TSN', 'USB', 'UBER', 'UDR', 'ULTA', 'UNP', 'UAL', 'UPS', 'URI', 'UNH', 'UHS', 'VLO', 'VTR', 'VLTO', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VTRS', 'VICI', 'V', 'VST', 'VMC', 'WRB', 'GWW', 'WAB', 'WBA', 'WMT', 'DIS', 'WBD', 'WM', 'WAT', 'WEC', 'WFC', 'WELL', 'WST', 'WDC', 'WY', 'WMB', 'WTW', 'WDAY', 'WYNN', 'XEL', 'XYL', 'YUM', 'ZBRA', 'ZBH', 'ZTS']\n"]}]},{"cell_type":"markdown","source":["## Data Collection and Initial Analysis\n","Download the adjusted closing price for all S&P 500 stocks for each day in the five year window between Jan 1, 2014 and December 31, 2018, inclusive."],"metadata":{"id":"koIrvF12wEIR"}},{"cell_type":"code","source":["# Download prices from Yahoo Finance\n","prices_list = []\n","\n","# IMPORTANT NOTE: Dates are set for pre-pandemic to avoid any unexplained turbulence.\n","data = yf.download(tickers, start='2014-01-01', end='2019-01-01')['Adj Close']\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"id":"kU6VRJRtEF2j","outputId":"941b416d-b0f6-4ce0-f0e8-50e83588e24a","executionInfo":{"status":"ok","timestamp":1735721693714,"user_tz":-480,"elapsed":87725,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[*********************100%***********************]  503 of 503 completed\n","ERROR:yfinance:\n","17 Failed downloads:\n","ERROR:yfinance:['KVUE', 'GEV', 'GEHC', 'SW', 'CEG', 'FOX', 'VLTO', 'DOW', 'FOXA', 'UBER', 'PLTR', 'CARR', 'ABNB', 'CTVA', 'CRWD', 'OTIS', 'SOLV']: YFPricesMissingError('$%ticker%: possibly delisted; no price data found  (1d 2014-01-01 -> 2019-01-01) (Yahoo error = \"Data doesn\\'t exist for startDate = 1388552400, endDate = 1546318800\")')\n"]},{"output_type":"execute_result","data":{"text/plain":["Ticker      ABNB  CARR  CEG  CRWD  CTVA  DOW  FOX  FOXA  GEHC  GEV  KVUE  \\\n","Date                                                                       \n","2014-01-02   NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   \n","2014-01-03   NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   \n","2014-01-06   NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   \n","2014-01-07   NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   \n","2014-01-08   NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   \n","\n","Ticker      OTIS  PLTR  SOLV  SW  UBER  VLTO  \n","Date                                          \n","2014-01-02   NaN   NaN   NaN NaN   NaN   NaN  \n","2014-01-03   NaN   NaN   NaN NaN   NaN   NaN  \n","2014-01-06   NaN   NaN   NaN NaN   NaN   NaN  \n","2014-01-07   NaN   NaN   NaN NaN   NaN   NaN  \n","2014-01-08   NaN   NaN   NaN NaN   NaN   NaN  "],"text/html":["\n","  <div id=\"df-24748c8d-d5aa-493d-90a5-cff968f5fc2c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Ticker</th>\n","      <th>ABNB</th>\n","      <th>CARR</th>\n","      <th>CEG</th>\n","      <th>CRWD</th>\n","      <th>CTVA</th>\n","      <th>DOW</th>\n","      <th>FOX</th>\n","      <th>FOXA</th>\n","      <th>GEHC</th>\n","      <th>GEV</th>\n","      <th>KVUE</th>\n","      <th>OTIS</th>\n","      <th>PLTR</th>\n","      <th>SOLV</th>\n","      <th>SW</th>\n","      <th>UBER</th>\n","      <th>VLTO</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2014-01-02</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-01-03</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-01-06</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-01-07</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2014-01-08</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24748c8d-d5aa-493d-90a5-cff968f5fc2c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-24748c8d-d5aa-493d-90a5-cff968f5fc2c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-24748c8d-d5aa-493d-90a5-cff968f5fc2c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-749db88c-83c1-44bf-9344-9474f3d3e14f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-749db88c-83c1-44bf-9344-9474f3d3e14f')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-749db88c-83c1-44bf-9344-9474f3d3e14f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 1258,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2014-01-02 00:00:00\",\n        \"max\": \"2018-12-31 00:00:00\",\n        \"num_unique_values\": 1258,\n        \"samples\": [\n          \"2016-03-28 00:00:00\",\n          \"2014-05-29 00:00:00\",\n          \"2014-03-18 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ABNB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CARR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CEG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CRWD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CTVA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DOW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FOX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FOXA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GEHC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GEV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"KVUE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"OTIS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PLTR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SOLV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UBER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VLTO\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["In our exploratory data analysis, we see that our stock prices dataset has 1258 rows (the number of trading days in a five year period) and 485 columns (we note that data for some of the tickers was unavailable for our chosen 5 year period, as some stocks were initially listed or delisted from the S&P 500 during this timeframe).\n","\n","We also noted that some tickers had null values for each date, so we decided to exclude these from our dataset."],"metadata":{"id":"a9guH-bAw0tI"}},{"cell_type":"code","source":["# Perform some EDA and learn about the data\n","print(data.info())\n","print(data.describe())\n","print(data.shape)\n","print(data.isnull().sum())\n","\n","# isnull().sum() found some tickers to be completely null, so we drop those assets and check again\n","data = data.dropna(axis=1, how='all')\n","print(data.isnull().sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V09rkMSN-Nq","outputId":"6749961f-925f-4d3a-d6fe-111613641a26","executionInfo":{"status":"ok","timestamp":1735721693714,"user_tz":-480,"elapsed":11,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","DatetimeIndex: 1258 entries, 2014-01-02 to 2018-12-31\n","Data columns (total 17 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   ABNB    0 non-null      float64\n"," 1   CARR    0 non-null      float64\n"," 2   CEG     0 non-null      float64\n"," 3   CRWD    0 non-null      float64\n"," 4   CTVA    0 non-null      float64\n"," 5   DOW     0 non-null      float64\n"," 6   FOX     0 non-null      float64\n"," 7   FOXA    0 non-null      float64\n"," 8   GEHC    0 non-null      float64\n"," 9   GEV     0 non-null      float64\n"," 10  KVUE    0 non-null      float64\n"," 11  OTIS    0 non-null      float64\n"," 12  PLTR    0 non-null      float64\n"," 13  SOLV    0 non-null      float64\n"," 14  SW      0 non-null      float64\n"," 15  UBER    0 non-null      float64\n"," 16  VLTO    0 non-null      float64\n","dtypes: float64(17)\n","memory usage: 176.9 KB\n","None\n","Ticker  ABNB  CARR  CEG  CRWD  CTVA  DOW  FOX  FOXA  GEHC  GEV  KVUE  OTIS  \\\n","count    0.0   0.0  0.0   0.0   0.0  0.0  0.0   0.0   0.0  0.0   0.0   0.0   \n","mean     NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","std      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","min      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","25%      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","50%      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","75%      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","max      NaN   NaN  NaN   NaN   NaN  NaN  NaN   NaN   NaN  NaN   NaN   NaN   \n","\n","Ticker  PLTR  SOLV   SW  UBER  VLTO  \n","count    0.0   0.0  0.0   0.0   0.0  \n","mean     NaN   NaN  NaN   NaN   NaN  \n","std      NaN   NaN  NaN   NaN   NaN  \n","min      NaN   NaN  NaN   NaN   NaN  \n","25%      NaN   NaN  NaN   NaN   NaN  \n","50%      NaN   NaN  NaN   NaN   NaN  \n","75%      NaN   NaN  NaN   NaN   NaN  \n","max      NaN   NaN  NaN   NaN   NaN  \n","(1258, 17)\n","Ticker\n","ABNB    1258\n","CARR    1258\n","CEG     1258\n","CRWD    1258\n","CTVA    1258\n","DOW     1258\n","FOX     1258\n","FOXA    1258\n","GEHC    1258\n","GEV     1258\n","KVUE    1258\n","OTIS    1258\n","PLTR    1258\n","SOLV    1258\n","SW      1258\n","UBER    1258\n","VLTO    1258\n","dtype: int64\n","Series([], dtype: float64)\n"]}]},{"cell_type":"markdown","source":["We now use our cleaned data to create a table that matches stock tickers with their annualized returns and volatility.  In the process of doing this, we look at the percent change of the adjusted closing price, effectively normalizing the data."],"metadata":{"id":"jbtkKcsAywYT"}},{"cell_type":"code","source":["# Create a new table with rows for assets, and columns for returns and volatility\n","daily_returns = data.pct_change().dropna()\n","annual_trading_days = 252\n","annualized_returns = daily_returns.mean() * annual_trading_days\n","annualized_volatility = daily_returns.std() * np.sqrt(annual_trading_days)\n","\n","# Create the returns DataFrame\n","returns = pd.DataFrame({\n","    'Ticker': data.columns,\n","    'Returns': annualized_returns.values,  # Use .values to avoid index alignment issues\n","    'Volatility': annualized_volatility.values\n","})\n","\n","print(data['AAPL'].head())  # First few rows\n","print(data['AAPL'].tail())\n","daily_returns.describe()\n","returns.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"id":"3bifutRGQDPC","outputId":"71c9ab9f-47ef-460e-9a50-ab8fafd9c298","executionInfo":{"status":"error","timestamp":1735722536852,"user_tz":-480,"elapsed":252,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":7,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'AAPL'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'AAPL'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-39c14cf71ba6>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m })\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# First few rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdaily_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'AAPL'"]}]},{"cell_type":"markdown","source":["## Data Visualization\n","Now let's take a dive deep to take a better look at the dataset that we are dealing with, and see if we and draw any interesting insights from it."],"metadata":{"id":"MooNGOD7I717"}},{"cell_type":"code","source":["# distribution of annualized returns\n","plt.figure(figsize=(10, 6))\n","sns.histplot(returns['Returns'], kde=True, bins=30, color='blue', edgecolor='black', alpha=0.7)\n","plt.title('Distribution of Annualized Returns Across All Stocks', fontsize=16, weight='bold')\n","plt.xlabel('Annualized Returns', fontsize=12)\n","plt.ylabel('Frequency', fontsize=12)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()"],"metadata":{"id":"L6HN-ptGL529","executionInfo":{"status":"aborted","timestamp":1735721693983,"user_tz":-480,"elapsed":13,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This histogram displays the distribution of annualized returns for all stocks in the dataset. It provides an overview of how the returns are spread, with a superimposed kernel density estimate (KDE) line to highlight the shape of the distribution.\n","\n","Key Insights:\n","\n","1. Concentration Around Negative Returns: Most stocks exhibit annualized returns clustered between -2% and -1%, suggesting a predominance of underperforming stocks during the analyzed period.\n","\n","2. Presence of Outliers: While the majority of returns are near the mean, there are a few stocks with significantly positive or negative returns, highlighting potential high-risk/high-reward investment opportunities."],"metadata":{"id":"C0wWHrbzMEzp"}},{"cell_type":"code","source":["# scatter Plot to show Annualized Returns vs Volatility\n","plt.figure(figsize=(12, 8))\n","scatter = plt.scatter(\n","    returns['Volatility'],\n","    returns['Returns'],\n","    c=returns['Returns'],\n","    cmap='viridis',\n","    s=100,\n","    edgecolor='black'\n",")\n","plt.colorbar(scatter, label=\"Annualized Returns\")\n","plt.title(\"Annualized Returns vs Volatility for Stocks\", fontsize=14)\n","plt.xlabel(\"Volatility (Annualized)\", fontsize=12)\n","plt.ylabel(\"Returns (Annualized)\", fontsize=12)\n","plt.grid(alpha=0.3)\n","\n","# ticker Annotations\n","for i, ticker in enumerate(returns['Ticker']):\n","    if i % 20 == 0:\n","        plt.annotate(ticker, (returns['Volatility'][i], returns['Returns'][i]), fontsize=8, ha='right')\n","\n","plt.show()"],"metadata":{"id":"h3Xz3bnqGb0x","executionInfo":{"status":"aborted","timestamp":1735721693984,"user_tz":-480,"elapsed":14,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The scatter plot visualizes the relationship between annualized returns (y-axis) and annualized volatility (x-axis) for various stocks, highlighting the risk-reward trade-off. Each dot represents a stock, with its position showing its risk level (volatility) and performance (returns). The color gradient adds a layer of insight, indicating the magnitude of returns, with brighter colors representing higher returns. This visualization helps identify clusters, high-return performers, and outliers in the dataset.\n","\n","Key Insights:\n","\n","1.   Risk-Reward Trade-off: Stocks like EQT and GE exhibit higher volatility with varying returns, reinforcing the classic risk-reward relationship where higher risk can lead to either higher or significantly lower returns.\n","\n","2.   High-Return Outliers: Stocks like AVGO and PANW demonstrate exceptionally high annualized returns with moderate volatility, making them potential candidates for favorable risk-adjusted investment opportunities.\n","\n","\n","\n","\n"],"metadata":{"id":"StxMbumoHxPG"}},{"cell_type":"code","source":["#Bar Chart of Top Performers\n","top_performers = returns.sort_values('Returns', ascending=False).head(10)\n","\n","plt.figure(figsize=(12, 6))\n","sns.barplot(x='Ticker', y='Returns', data=top_performers, palette='coolwarm')\n","plt.title('Top 10 Stocks by Annualized Returns', fontsize=16)\n","plt.xlabel('Ticker', fontsize=12)\n","plt.ylabel('Annualized Returns', fontsize=12)\n","plt.xticks(rotation=45)\n","plt.show()"],"metadata":{"id":"bOI5_WKmJ5LA","executionInfo":{"status":"aborted","timestamp":1735721693984,"user_tz":-480,"elapsed":13,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This bar chart visualizes the top 10 stocks with the highest annualized returns from the dataset. Each bar represents a stock (identified by its ticker symbol) and its corresponding annualized return, showcasing the best-performing assets in terms of growth over the analyzed period.\n","\n","Key Insights:\n","\n","1. Top Performer: Broadcom Inc. (AVGO) significantly outperformed other stocks with the highest annualized return, exceeding 2.0. This indicates its exceptional growth potential compared to its peers.\n","\n","2. Consistent High Returns: General Electric (GE) and Dollar Tree (DLTR) follow as strong performers, both showing annualized returns above 1.5, highlighting their consistent performance in the market.\n","\n","3. Sector Representation: The top 10 stocks represent diverse sectors, suggesting opportunities for high returns across industries rather than concentration in a single market segment."],"metadata":{"id":"uZTyXJQdLI2T"}},{"cell_type":"markdown","source":["## Clustering\n","In this part of our analysis we use K-Means clustering to attempt to group the data in 1 to 10 clusters.\n","\n","To determine the appropriate number of clusters, we plot the within-cluster sum of squares as a function of the number of clusters and look for the elbow."],"metadata":{"id":"Ht3R8mVU-I4e"}},{"cell_type":"code","source":["#Apply K-Means clustering, first use elbow method to determine clusters\n","clustering_data = returns[['Ticker', 'Returns', 'Volatility']].copy()\n","\n","# List to store the within-cluster sum of squares (WCSS) for each number of clusters\n","wcss = []\n","\n","# Calculate WCSS for different numbers of clusters\n","for k in range(1, 11):  # Try 1 to 10 clusters\n","    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n","    kmeans.fit(clustering_data[['Returns', 'Volatility']])\n","    wcss.append(kmeans.inertia_)\n","\n","# Plot the Elbow Curve\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, 11), wcss, linestyle='--')\n","plt.title('Elbow Method for Optimal Number of Clusters')\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n","plt.xticks(range(1, 11))\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"fZNykOgsGGMy","executionInfo":{"status":"aborted","timestamp":1735721693984,"user_tz":-480,"elapsed":13,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From the plot we can see that there is an elbow at three clusters, indicating that we should group our data into this number of clusters.\n","\n","We next use the silhouette score, which measures how similar data points in a cluster are to each other, to verify the optimal number of clusters."],"metadata":{"id":"QcZfB8Oy-6fg"}},{"cell_type":"code","source":["# Also try to confirm with the silhouette_score\n","\n","from sklearn.metrics import silhouette_score\n","\n","silhouette_scores = []\n","for k in range(2, 11):  # Silhouette score is undefined for k=1\n","    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n","    kmeans.fit(clustering_data[['Returns', 'Volatility']])\n","    score = silhouette_score(clustering_data[['Returns', 'Volatility']], kmeans.labels_)\n","    silhouette_scores.append(score)\n","\n","# Plot the silhouette scores\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')\n","plt.title('Silhouette Score for Optimal Number of Clusters')\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('Silhouette Score')\n","plt.xticks(range(2, 11))\n","plt.grid()\n","plt.show()\n"],"metadata":{"id":"Rkdvl3pATELH","executionInfo":{"status":"aborted","timestamp":1735721693984,"user_tz":-480,"elapsed":12,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This plot further confirms our conclusion that the optimal number of clusters for our data is 3.\n","\n","Next we visualize the clusters in a scatter plot."],"metadata":{"id":"Qgo38AYx_nq7"}},{"cell_type":"code","source":["# Optimal number of clusters is 3, confirmed by the elbow and silhouette chart\n","kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n","kmeans.fit(clustering_data[['Returns', 'Volatility']])\n","clustering_data['Cluster'] = kmeans.labels_\n","\n","import seaborn as sns\n","\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(data=clustering_data, x='Returns', y='Volatility', hue='Cluster', palette='viridis')\n","plt.title('K-Means Clustering Results')\n","\n","centroids = kmeans.cluster_centers_\n","plt.scatter(centroids[:, 0], centroids[:, 1],\n","            s=300, c='red', marker='X', label='Centroids')\n","\n","clustering_data['Cluster'].value_counts()\n"],"metadata":{"id":"vb3O_vMqar6n","executionInfo":{"status":"aborted","timestamp":1735721693984,"user_tz":-480,"elapsed":12,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cluster Analysis and Portfolio Optimization\n","\n","Here we analyze each cluster in terms of its makeup and performance, in order to set parameters to create a diversified but high performing portfolio.\n","\n","We use the yfinance API to retrieve the sector for each stock ticker and merge it to our clustering_data dataframe."],"metadata":{"id":"TcqUtN0vZemf"}},{"cell_type":"markdown","source":["### **Disclaimer: yfinance can be unstable when pulling data. If the code takes too long, please restart the runtime and try again. Thank you!*"],"metadata":{"id":"39A0WTxJLFDp"}},{"cell_type":"code","source":["# Suppress yfinance logging\n","yf_logger = logging.getLogger(\"yfinance\")\n","yf_logger.setLevel(logging.CRITICAL)\n","\n","# Fetch sectors without printing errors - retry until it works (will only take 6-7 mins max)\n","sectors = {}\n","for ticker in tickers:\n","    try:\n","        stock = yf.Ticker(ticker)\n","        info = stock.info\n","        sectors[ticker] = info.get('sector', 'Unknown')\n","    except Exception:\n","        sectors[ticker] = 'Unknown'\n","\n","sector_df = pd.DataFrame(list(sectors.items()), columns=['Ticker', 'Sector'])\n","sector_df.head()"],"metadata":{"id":"BuSK76CynOIA","executionInfo":{"status":"aborted","timestamp":1735721693985,"user_tz":-480,"elapsed":13,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}},"collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering_data = clustering_data.reset_index().merge(sector_df, on='Ticker', how='left')\n","clustering_data.head()"],"metadata":{"id":"4UizG1tFnb2h","executionInfo":{"status":"aborted","timestamp":1735721693985,"user_tz":-480,"elapsed":12,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count the sectors in each cluster\n","sector_counts = clustering_data.groupby(['Cluster', 'Sector']).size().reset_index(name='Count')\n"],"metadata":{"id":"tWj9-PBuouOc","executionInfo":{"status":"aborted","timestamp":1735721693985,"user_tz":-480,"elapsed":12,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(12, 6))\n","sns.barplot(data=sector_counts, x='Cluster', y='Count', hue='Sector')\n","plt.title('Sector Distribution by Cluster')\n","plt.xlabel('Cluster')\n","plt.ylabel('Number of Stocks')\n","plt.legend(title='Sector')\n","plt.show()\n"],"metadata":{"id":"dNUxZQV_o3vM","executionInfo":{"status":"aborted","timestamp":1735721693985,"user_tz":-480,"elapsed":11,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The K-Means clustering showed that cluster 0 was the best performing while cluster 2 was the worst performing."],"metadata":{"id":"xIzE0dqjpC4-"}},{"cell_type":"markdown","source":["We will now analyze the sector characteristics within each cluster. Understanding the differences in performance of each sector within the cluster is very helpful when building a portfolio"],"metadata":{"id":"7nfcng_1pvrp"}},{"cell_type":"code","source":["sector_performance = clustering_data.groupby(['Cluster', 'Sector']).agg(\n","    Avg_Returns=('Returns', 'mean'),\n","    Avg_Volatility=('Volatility', 'mean')\n",").reset_index()\n"],"metadata":{"id":"7gl5zY5jo5WN","executionInfo":{"status":"aborted","timestamp":1735721693985,"user_tz":-480,"elapsed":11,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make a heatmap to compare average returns and volatility by sector by cluster\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(sector_performance.pivot(index='Cluster', columns='Sector', values='Avg_Returns'), annot=True, cmap='coolwarm')\n","plt.title('Average Returns by Sector and Cluster')\n","plt.xlabel('Sector')\n","plt.ylabel('Cluster')\n"],"metadata":{"id":"ZOJ5mxqiqB-U","executionInfo":{"status":"aborted","timestamp":1735721693987,"user_tz":-480,"elapsed":13,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As Mentioned earlier, cluster 2 holds the poorest performing stocks, and this holds true accross every sector. It would be ideal to avoid this cluster alltogether when choosing an investment strategy, since it is clear that the stocks in this segment will not perform well.\n","\n","Conversely, cluster 0 performed the best in all sectors, so it would be wise to select assets that fall into that cluster.\n","\n","Based on this heatmap, for the time frame selected it would be ideal to select assets that are within the Consumer Cyclical, Industrials, and Consumer Defensive sectors within cluster 0.\n","\n","It is also important to note that while the Consumer Defensive sector performed the best in cluster 0, it was one of the poorest in both cluster 1 and 2."],"metadata":{"id":"3P0Al3AvqfpO"}},{"cell_type":"code","source":["cluster_0_stocks = clustering_data[clustering_data['Cluster'] == 0]['Ticker'].tolist()\n","cluster_0_stocks"],"metadata":{"id":"2SPO9DW8qDg4","collapsed":true,"executionInfo":{"status":"aborted","timestamp":1735721693987,"user_tz":-480,"elapsed":116608,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Portfolio Optimiazation\n","\n","Now that we have identified the best cluster to invest in, we will create a tool to create weighted portfolio designed to optimize returns according to user-set parameters of risk tolerance and the maximum share a single stock should have in the portfolio.\n","\n","First we download the data for stocks in our desired cluster and calculate their mean returns over our chosen time period."],"metadata":{"id":"WmX7pST_phc9"}},{"cell_type":"code","source":["data = yf.download(cluster_0_stocks, start='2014-01-01', end='2019-01-01')['Adj Close']\n","returns  = data.pct_change().dropna()\n","mean_returns = returns.mean().values\n","cov_matrix = returns.cov()"],"metadata":{"id":"tQHMnBj6u1hr","executionInfo":{"status":"aborted","timestamp":1735721693987,"user_tz":-480,"elapsed":116607,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we use a the CVXpy python library to solve the constraint problem that will create a portfolio that allocate investments from our optimal cluster to simultaneously maximize returns while also constraining risk to a level set by the investor."],"metadata":{"id":"BiG5GtFbql8U"}},{"cell_type":"code","source":["import cvxpy as cp\n","import numpy as np\n","import pandas as pd\n","\n","# Number of assets\n","num_assets = len(cluster_0_stocks)\n","\n","# Ensure mean_returns is a NumPy array\n","mean_returns = mean_returns\n","\n","# Define optimization variables\n","weights = cp.Variable(num_assets)\n","\n","# Define risk and return\n","risk = cp.quad_form(weights, cov_matrix.values)  # Ensure cov_matrix is a NumPy array\n","expected_return = mean_returns.T @ weights  # Matrix multiplication\n","\n","# Optimization setup\n","risk_aversion = 2  # risk tolerance, raise to diversify to more stocks\n","objective = cp.Maximize(expected_return - risk_aversion * risk)\n","\n","constraints = [\n","    cp.sum(weights) == 1,  # Weights sum to 1\n","    weights >= 0,          # Long-only portfolio\n","    weights <= 0.2        # Maximum weight constraint of single asset\n","]\n","\n","# Solve the optimization problem\n","problem = cp.Problem(objective, constraints)\n","problem.solve()\n","\n","# Get optimal weights\n","optimal_weights = weights.value\n","\n","# Create a pandas Series for the weights\n","portfolio_weights = pd.Series(optimal_weights, index=cluster_0_stocks)\n","portfolio_weights[portfolio_weights < 0] = 0\n","threshold = 1e-6\n","portfolio_weights[portfolio_weights < threshold] = 0\n","portfolio_weights /= portfolio_weights.sum()\n","print(portfolio_weights[portfolio_weights>0])\n"],"metadata":{"id":"dKCrplm18-Dg","executionInfo":{"status":"aborted","timestamp":1735721693988,"user_tz":-480,"elapsed":116607,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have weighted our portfolio to be optimized for returns based on our desired parameters, we calculate the annual return for the portfolio by multiplying the stocks' returns by their optimal weights (i.e. the fraction of the portfolio comprised of them) and the number of trading days in a year, 252.\n","\n","We also calculate the volatility as the standard deviation of the returns (in this case multiplied by the square root of the number of trading days in order to annualize them), and the sharp ratio, which is the ratio of a portfolio's returns to its volatility."],"metadata":{"id":"gsfxCzX5rVZ-"}},{"cell_type":"code","source":["# Portfolio returns\n","portfolio_returns = (returns @ optimal_weights)\n","\n","# Annualized metrics\n","annual_return = np.mean(portfolio_returns) * 252\n","annual_volatility = np.std(portfolio_returns) * np.sqrt(252)\n","sharpe_ratio = annual_return / annual_volatility\n","\n","print(f\"Annual Return: {annual_return:.2%}\")\n","print(f\"Annual Volatility: {annual_volatility:.2%}\")\n","print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n"],"metadata":{"id":"Y9WjCarL9fQI","executionInfo":{"status":"aborted","timestamp":1735721693988,"user_tz":-480,"elapsed":116606,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To visualize our portfolio, we create a pie plot showing the stocks in our portfolio along with the percentage of the portfolio each asset comprises."],"metadata":{"id":"NeqXXw7Hs7HE"}},{"cell_type":"code","source":["positive_portfolio_weights = portfolio_weights[portfolio_weights>0]\n","\n","plt.figure(figsize=(8, 8))\n","positive_portfolio_weights.plot.pie(\n","    title=\"Portfolio Allocation\",\n","    ylabel='',          # Hide the y-axis label\n","    fontsize=12,        # Adjust font size\n","    autopct=lambda p: round(p, 1)\n",")\n","\n","# Display the chart\n","plt.show()"],"metadata":{"id":"z5clzHoQ_59a","executionInfo":{"status":"aborted","timestamp":1735721693988,"user_tz":-480,"elapsed":116606,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Above chart indicates the optimized portfolio allocation of cluster 0 stocks. It is the ideal distribution to minimize risk and maximize rewards during the 2014-2019 period that we are analyzing."],"metadata":{"id":"1MT0jbkeD3S2"}},{"cell_type":"markdown","source":["# Part Two: Sentiment Analysis"],"metadata":{"id":"VHQZeY4PC4U5"}},{"cell_type":"markdown","source":["The second portion of our project is implementation of an automonous trading bot based on sentiment analysis.\n","\n","We will combine two datasets found on Kaggle (links provided below).  One dataset has daily stock market data on the most watched stocks on yahoo finance over a one year time period, and the other dataset has the text of over 80,000 tweets about different stocks along with the date of the tweet and the stock ticker of the stock that it references.\n","\n","We will attempt to build a model that can predict whether a stock's price will rise or fall on a given day based on the average sentiment of tweets about this stock on that particular day.\n","\n"],"metadata":{"id":"YOQ0IdUwEc6S"}},{"cell_type":"markdown","source":["## Data Cleaning and Sentiment Analysis\n","Link to datasets:\n","https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction?resource=download&select=stock_yfinance_data.csv"],"metadata":{"id":"nKC4XucWMBB6"}},{"cell_type":"code","source":["# Get files from Drive\n","from google.colab import drive\n","import pandas as pd\n","drive.mount('/content/drive')\n","\n","tweet_data_path = '/content/drive/MyDrive/545 Project/stock_tweets.csv'\n","stock_data_path = '/content/drive/MyDrive/545 Project/stock_yfinance_data.csv'\n","\n","# The data came from Kaggle: https://www.kaggle.com/datasets/equinxx/stock-tweets-for-sentiment-analysis-and-prediction?resource=download&select=stock_yfinance_data.csv\n"],"metadata":{"id":"kutdLzAhb_eS","executionInfo":{"status":"aborted","timestamp":1735721693989,"user_tz":-480,"elapsed":116606,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we read in the two datasets.  We see that the tweet data has over 80,000 rows, and the stock data has over 6000 rows."],"metadata":{"id":"LX1usQjD3e3Z"}},{"cell_type":"code","source":["stock_data = pd.read_csv(stock_data_path)\n","tweet_data = pd.read_csv(tweet_data_path)\n","print(tweet_data.shape)\n","print(stock_data.shape)\n","tweet_data.head()"],"metadata":{"id":"wR8yNNzri6M1","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":36,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(stock_data.shape)\n","stock_data.head()"],"metadata":{"id":"LvbtSslIjDjk","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":35,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To clean the stock data, we take the following steps:\n","\n","\n","1.   Cast the 'Date' column into a datetime object and sort by date\n","2.   Rename columns\n","3.   Drop missing values.  Because the movements of stocks from day to day are not predictable and do not necessarily form any kind of patttern, we decided this is a better option than filling these values with something like a mean or median value.\n","\n"],"metadata":{"id":"Wkg3UwAzkuzj"}},{"cell_type":"code","source":["# Clean Stock Data\n","stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n","stock_data = stock_data.sort_values(by='Date')\n","stock_data.rename(columns={'Stock Name': 'Ticker'}, inplace=True)\n","\n","# Drop missing values\n","stock_data.dropna(inplace=True)"],"metadata":{"id":"ESzpfWM7kp0y","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":35,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To clean the tweet data, we take the following steps:\n","\n","\n","1.   Cast the 'Date' column to a datetime object\n","2.   Drop null values.  Since we are interested in the sentiment of the tweet, the date it was made, and the associated stock ticker, if a row is missing any information it will not help us in building our model.\n","3.   Clean the tweet text and put in in a new column 'clean_text'\n","\n","    1.   Remove symbols for hashtags (#) and mentions (@),\n","    2.   Remove any other special characters and whitespace\n","4.   Drop rows with empty string tweets or duplicate tweets.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"_xZdOvrU4dj1"}},{"cell_type":"code","source":["\n","# Clean Tweet Data\n","import re\n","\n","tweet_data['Date'] = pd.to_datetime(tweet_data['Date'], errors = 'coerce')\n","tweet_data.dropna(inplace=True)\n","def clean_tweet(tweet):\n","    # Remove URLs\n","    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n","\n","    # Remove menitons and hashtags\n","    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n","\n","    # Remove special characters\n","    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n","\n","    # Remove extra whitespaces\n","    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n","\n","    return tweet.strip()\n","\n","tweet_data['clean_text'] = tweet_data['Tweet'].apply(clean_tweet)\n","\n","# Drop Missing values and duplicate tweets\n","tweet_data.dropna(inplace=True)\n","tweet_data.drop_duplicates(subset='clean_text', inplace=True)\n","tweet_data.rename(columns={'Stock Name': 'Ticker'}, inplace=True)\n","tweet_data.head()"],"metadata":{"id":"2RHhOIw2k8Z0","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we use nltk's vader_lexicon's Sentiment Intensity Analyzer to compute the polarity score of each tweet."],"metadata":{"id":"WIiUk1qc5feR"}},{"cell_type":"code","source":["# Calculate sentiment scores for tweets\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import nltk\n","nltk.download('vader_lexicon')\n","\n","sid = SentimentIntensityAnalyzer()\n","tweet_data['Sentiment'] = tweet_data['clean_text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n","tweet_data.head()"],"metadata":{"id":"7eaSCIBIls5I","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(tweet_data))\n","cleaned_tweet_data = tweet_data.dropna()\n","print(len(cleaned_tweet_data))"],"metadata":{"id":"rhBTDFLzxYVi","executionInfo":{"status":"aborted","timestamp":1735721694451,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we aggregate the average sentiment scores on stock ticker and date, so in our final dataframe we will have an average daily sentiment for each stock ticker for each day."],"metadata":{"id":"QSudA8Iz5u2H"}},{"cell_type":"code","source":["# Aggregate sentiments to daily\n","tweet_data['Date'] = pd.to_datetime(tweet_data['Date']).dt.date\n","daily_sentiment = tweet_data.groupby(['Ticker', 'Date'])['Sentiment'].mean().reset_index()\n","print(daily_sentiment.shape)\n","tweet_data.head()"],"metadata":{"id":"-hQhvDajuBqc","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally we merge the stock data with the daily twitter sentiment data.  We do a left merge, to include ticker/day combinations where there are no associated tweets and therefore no associated sentiment.  We also add a column for the next day's closing price, which we then use to determine whether the stock's price rose or fell on a given date.  We then make a new column named 'Target' that has a value of 1 if the price rose and 0 if the price did not (which we will use for logistic regression), and a column named 'One_Day_Change', which calculates the percentage change in stock price over the next day (which we will use for linear regression)."],"metadata":{"id":"NiwcfeU099U2"}},{"cell_type":"code","source":["stock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.date\n","stock_data = stock_data[['Ticker', 'Date', 'Adj Close', 'Volume']].dropna()\n","print(stock_data.shape)"],"metadata":{"id":"BXBrIb9lvMls","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge the twitter sentiment with the stock data\n","combined_data = pd.merge(stock_data, daily_sentiment, on=['Ticker', 'Date'], how='left')\n","\n","# Create a new col for next day adjusted close by shifting by -1\n","combined_data['Next_Day_Adj_Close'] = combined_data.groupby('Ticker')['Adj Close'].shift(-1)\n","\n","# Create a new col to track if the price went up. Our bot assumes only long positions (no short selling)\n","combined_data['Target'] = (combined_data['Next_Day_Adj_Close'] > combined_data['Adj Close']).astype(int)\n","combined_data['One_Day_Change'] = ((combined_data['Next_Day_Adj_Close'] - combined_data['Adj Close']) / combined_data['Adj Close'])\n","\n","# Drop any rows with NaN targets\n","combined_data.dropna(subset=['Target'], inplace=True)\n","combined_data.dropna(subset=['One_Day_Change'], inplace=True)\n","\n","\n","combined_data.head()"],"metadata":{"id":"V9d3Q00tvscN","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":34,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we repalce any NaN sentiment values with a value of 0, which is a neutral sentiment."],"metadata":{"id":"pKeL8roz_MRY"}},{"cell_type":"code","source":["combined_data = combined_data.fillna(0)"],"metadata":{"id":"P35A6oP5DjP_","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":33,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["combined_data.head()"],"metadata":{"id":"Ylj6l_qeFe1p","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":33,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualizing the Data\n","\n","We plot a frequency plot of the sentiment scores of our data to see how they are distributed, which will inform our decision of how to scale our data in our models."],"metadata":{"id":"lwVASaT14UWf"}},{"cell_type":"code","source":["sent = plt.hist(combined_data['Sentiment'])\n","plt.title('Tweet Sentiment Distribution')\n","plt.xlabel(\"Tweet Sentiment\")\n","plt.ylabel(\"Number of Ticker/Day Pairs\")"],"metadata":{"id":"1VWH80fk_Xy3","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":33,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adj_close = plt.hist(combined_data['Adj Close'])\n","plt.title('Adjusted Closing Price Distribution')\n","plt.xlabel(\"Adjusted Closing Price\")\n","plt.ylabel(\"Number of Ticker/Day Pairs\")"],"metadata":{"id":"zP49zDAY9jCX","executionInfo":{"status":"aborted","timestamp":1735721694452,"user_tz":-480,"elapsed":32,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["volume = plt.hist(combined_data['Volume'])\n","plt.title('Volume Distribution')\n","plt.xlabel(\"Volume\")\n","plt.ylabel(\"Number of Ticker/Day Pairs\")"],"metadata":{"id":"Q5OHibZT9lz9","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":33,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that our data is only normally distributed, more or less, in the sentiment category, and that the other categories are heavily skewed towards lower values.  Because of this, we will use the MinMax scaler instead of the Standard Scaler to preserve the shape of the data."],"metadata":{"id":"UyRktWKW9rBT"}},{"cell_type":"markdown","source":["## Building the models\n","\n","Now that daily sentiments for each ticker are calculated, we can begin to put together a model to use the sentiment scores of the tweets to predict the movement of a stock's price. We will test various models, including logistic regression, random forest regression, and a neural network built with tensorflow and the Adam optimizer."],"metadata":{"id":"njwsV2sPtkhZ"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","import sklearn.metrics"],"metadata":{"id":"aveS-K7Qv0pH","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":33,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we create our feature set, comprising of adjusted closing price, trading volume, and average daily sentiment. For our target variable, we use 'One_Day_Change' for our linear regression models and 'Target' for our logisitic regression models.  We then split the data into training and test sets, with the test set being 20% of the original dataset, and scale the data."],"metadata":{"id":"UED2QzrS_uNM"}},{"cell_type":"code","source":["print(len(combined_data))\n","combined_data_cleaned = combined_data.dropna()\n","print(len(combined_data_cleaned))\n","X_log = combined_data_cleaned[['Adj Close', 'Volume', 'Sentiment']]\n","y_log = combined_data_cleaned['Target']\n","X_lin = combined_data_cleaned[['Adj Close', 'Volume', 'Sentiment']]\n","y_lin = combined_data_cleaned['One_Day_Change']\n","\n","scaler = MinMaxScaler()\n","# scaler= StandardScaler()\n","\n","# Split the data\n","X_train_lin, X_test_lin, y_train_lin, y_test_lin = train_test_split(X_lin, y_lin, test_size=0.2, random_state=42)\n","\n","X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log, y_log, test_size=0.2, random_state=42)\n","\n","\n","# Scale the data\n","X_train_lin = scaler.fit_transform(X_train_lin)\n","X_test_lin = scaler.fit_transform(X_test_lin)\n","\n","X_train_log = scaler.fit_transform(X_train_log)\n","X_test_log = scaler.fit_transform(X_test_log)"],"metadata":{"id":"cPafn9pnGRG0","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":30,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Linear Regression**\n","\n","A Linear Regression model is fitted on the scaled data to predict One_Day_Change. The performance is evaluated using the coefficient of determination."],"metadata":{"id":"cfnFpG3whyvq"}},{"cell_type":"code","source":["lin_reg = LinearRegression()\n","lin_reg.fit(X_train_lin, y_train_lin)\n","\n","y_pred = lin_reg.predict(X_test_lin)"],"metadata":{"id":"0b_oRxoegWj7","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":29,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To evaluate our linear regression, we use the score feature of scikitlearn's linear regression model.  This calculates the coeffient of determination, defined as one minus the ratio of the residual sum of squares to the total sum of squares.\n","\n","In our model, the coefficient is negative, which is a poor result, essentially saying that the model is (in this case slightly) worse than just assigning a the mean of the data to each data point.\n","\n","Given this poor result, we will try to create a better model using other statistical methods."],"metadata":{"id":"y8c564LdjfiI"}},{"cell_type":"code","source":["lin_reg_score = lin_reg.score(X_test_lin, y_test_lin)\n","print(lin_reg_score)"],"metadata":{"id":"z-c13shZgdDj","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":28,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Logistic Regression**\n","\n","A Logistic Regression model is trained to classify price movement (up or down). The confusion matrix and accuracy score evaluate its performance."],"metadata":{"id":"kf3W5bVUk4jS"}},{"cell_type":"code","source":["log_clf = LogisticRegression()\n","log_clf.fit(X_train_log, y_train_log)"],"metadata":{"id":"SL0OPqvZugyH","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":28,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After fitting the model, we use it to make predictions on our test data, create a confusion matrix, and calculate the accuracy of our model.\n","\n","Looking at the confusion matrix, we can see that our model is predicting a value of 0 for every datapoint (i.e. that for all data points the price of the stock will go down or remain constant), making the model completely unusable."],"metadata":{"id":"5MqUBSxQlhh5"}},{"cell_type":"code","source":["prediction = log_clf.predict(X_test_log)\n","\n","log_confusion = sklearn.metrics.confusion_matrix(y_test_log, prediction)\n","print(log_confusion)\n","disp = ConfusionMatrixDisplay(confusion_matrix=log_confusion,\n","                              display_labels=log_clf.classes_)\n","disp.plot()\n","\n","log_acc = sklearn.metrics.accuracy_score(prediction,y_test_log)\n","print(log_acc)"],"metadata":{"id":"jRkvfGiFujDr","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":28,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["At this point, after noting the low performance of our linear and logistic regression models, we discussed the idea of using PCA, lasso, or ridge regression to improve performance, but as we only had a total of 3 features being used to predict our target, we decided that in this instance these models would likely not be beneficial."],"metadata":{"id":"rWGRt6VMQkrC"}},{"cell_type":"markdown","source":["### Random Forest Classifier\n","\n","Having noted the extremely poor performance of our vanilla logistic regression classifier, we will try to use a random forest classifier to create a better model.  It will also have the advantage of providing us the importance it assigns to different variables, which could aid us in removing variables that may be making our data noisy and harder to predict.\n","\n","We fit a random forest with 200 trees and a maximum depth of 60 in the hope that this large number of deep trees will create a better classifier."],"metadata":{"id":"sQimL239hzg2"}},{"cell_type":"code","source":["rf_clf = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=60, class_weight='balanced')\n","\n","rf_clf.fit(X_train_log,y_train_log)\n","\n","y_pred = rf_clf.predict(X_test_log)\n","\n","rf_acc = rf_clf.score(X_test_log, y_test_log)\n","print(rf_acc)"],"metadata":{"id":"4je_otA0yW9o","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfortunately, our random forest classifier had an accuracy even lower than our regular logistic regression classifier.  Upon analyzing the outputs however, we note one improvment: the classifier is now assigning values of both 1 and 0 to our test data.\n","\n","The random forest classifier also allows us to look at the importance of each variable in the model, and all variables have roughly the same importance.  Given that we expect, from our real world knowledge of markets, sentiment to have a larger influence on stock prices than raw closing price, this is a sign that our model is not finding any correlation between our features and targets."],"metadata":{"id":"KbEZEOaopBNI"}},{"cell_type":"code","source":["rf_confusion = sklearn.metrics.confusion_matrix(y_test_log, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=rf_confusion,\n","                              display_labels=rf_clf.classes_)\n","disp.plot()\n","\n","importances = rf_clf.feature_importances_\n","print(importances)"],"metadata":{"id":"pxqzkH8CobVu","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Neural Network**\n","\n","A simple Neural Network is built using TensorFlow/Keras with two hidden layers and trained using the Adam optimizer IN an attempt to increase the accuracy of the prediction."],"metadata":{"id":"ijbVMyEAiVJF"}},{"cell_type":"code","source":["X = combined_data[['Adj Close', 'Volume', 'Sentiment']]\n","y = combined_data['Target']\n","\n","# Scaling Feature\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Building the neural network\n","model = Sequential([\n","    Dense(16, input_dim=X_train.shape[1], activation='relu'),\n","    Dense(8, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])"],"metadata":{"id":"9LoNbdYBxuev","executionInfo":{"status":"aborted","timestamp":1735721694453,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n","\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"],"metadata":{"id":"C3iVFOD-yIJH","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":28,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.any(np.isnan(X_scaled)), np.any(np.isinf(X_scaled)))  # Check features\n","print(np.any(np.isnan(y)), np.any(np.isinf(y)))               # Check labels\n"],"metadata":{"id":"FA5Pael_yIfv","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":28,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our neural network using the Adam optimizer received a slightly lower accuracy score than our random forest model, with an accuracy of only 0.5108"],"metadata":{"id":"5fGgpbUrqC6z"}},{"cell_type":"markdown","source":["### **XGBoost Classifier**\n","\n","The XGBoost Classifier is tested and optimized using grid search for hyperparameter tuning."],"metadata":{"id":"srGpELpNi0pu"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.model_selection import GridSearchCV\n","\n","X_log_clean = combined_data_cleaned[['Adj Close', 'Volume', 'Sentiment']]\n","y_log_clean = combined_data_cleaned['Target']\n","\n","scaler = MinMaxScaler()\n","X_scaled = scaler.fit_transform(X_log_clean)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_log_clean, test_size=0.2, random_state=42)"],"metadata":{"id":"6FYNgUAT47t1","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xgb_clf = XGBClassifier(\n","    use_label_encoder=False,\n","    eval_metric='logloss',\n","    random_state=42\n",")\n","xgb_clf.fit(X_train, y_train)"],"metadata":{"id":"qSt9s73V5la0","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred = xgb_clf.predict(X_test)\n","acc = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {acc:.4f}\")\n","\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb_clf.classes_)\n","disp.plot()"],"metadata":{"id":"Oze0FmZbL1VZ","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0],\n","}\n","\n","grid_search = GridSearchCV(\n","    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n","    param_grid=param_grid,\n","    scoring='accuracy',\n","    cv=3,\n","    verbose=1\n",")\n","\n","grid_search.fit(X_train, y_train)\n","best_params = grid_search.best_params_\n","print(\"Best Parameters:\", best_params)"],"metadata":{"id":"NBhxIGBhL7KY","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimized_xgb = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)\n","optimized_xgb.fit(X_train, y_train)\n","\n","y_pred_opt = optimized_xgb.predict(X_test)\n","acc_opt = accuracy_score(y_test, y_pred_opt)\n","print(f\"Optimized Accuracy: {acc_opt:.4f}\")\n","\n","cm_opt = confusion_matrix(y_test, y_pred_opt)\n","disp_opt = ConfusionMatrixDisplay(confusion_matrix=cm_opt, display_labels=optimized_xgb.classes_)\n","disp_opt.plot()"],"metadata":{"id":"GP-gcWy3Mea4","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Performance Summary and comments"],"metadata":{"id":"x0HcP-dnm2I9"}},{"cell_type":"code","source":["#Model Performance Comparison\n","models = ['Logistic Regression', 'Random Forest', 'Neural Network', 'XGBoost (Vanilla)', 'XGBoost (Optimized)']\n","accuracies = [0.5179, 0.5187, 0.5211, 0.5195, 0.5116]\n","\n","plt.figure(figsize=(10, 6))\n","plt.bar(models, accuracies, color='skyblue')\n","plt.title('Model Performance Comparison')\n","plt.xlabel('Models')\n","plt.ylabel('Accuracy (or $R^2$ for Linear Regression)')\n","plt.xticks(rotation=45, ha='right')\n","plt.axhline(y=0.5, color='r', linestyle='--', label='Random Baseline')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"-O4n7EFSmrnm","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["** Note that we do not include our linear regression model here, as it is scored not on accuracy but by a coefficient of determination, which ranges from -1 to 1, which is not comparable to the accuracy scores of the logistic models."],"metadata":{"id":"n7bamSjGbqSn"}},{"cell_type":"markdown","source":["| **Model**                     | **Coefficient of Determination** |\n","|-------------------------------|--------------------|\n","| **Linear Regression**        |   -0.0011\n","\n","The result of our linear regression shows that our model is slightly worse than a coin flip at predicting correctly the price of stocks in our dataset."],"metadata":{"id":"JUoywBuhN8sl"}},{"cell_type":"markdown","source":["| **Model**                     | **Accuracy** |\n","|-------------------------------|--------------------|\n","| **Logistic Regression**        | 0.5179            |\n","| **Random Forest**              | 0.5187            |\n","| **Neural Network (NN)**        | 0.5211            |\n","| **XGBoost (Vanilla)**          | 0.5195            |\n","| **XGBoost (Optimized)**        | 0.5116            |\n","\n","The results of the models demonstrate that, while there may be *some* underlying correlation between tweet sentiment and priced movements, the models we implmented present at best solid foundation for further research and development in leveraging sentiment analysis and market data to predict stock price movements, rather than a usable tool for a serious or profit-conscious investor. With accuracies are just above that of a coin flip, they do not reveal reliable insights into the predictive potential of social media sentiment analysis in stock price forecasting. Below is a summary of key  conclusions drawn from each model:\n","\n","\n","1. **Logistic Regression**  \n","   Logistic regression achieved an accuracy of **51.79%**. This result suggests that this basic model cannot extract meaningfully predictive signals from a combination of market data and sentiment analysis.\n","\n","2. **Random Forest**  \n","   With an accuracy of **51.87%**, the random forest model achieved a similarly low accuracy as the naive logistic regression. This result indicates that, at least based on the data we have used, there is likely no significant correlation between tweet sentiment and price movements.\n","\n","3. **Neural Network (NN)**  \n","   Although it achieved the highest accuracy of **52.11%**, the neural network underscores that our data is not showing a correlation between our variables and price changes, as even an advanced architecture did not uncover correlation in the data.\n","\n","4. **XGBoost Models**  \n","   Both vanilla and optimized XGBoost models performed similarly to our other, more naive models, achieving accuracies of **51.95%** and **51.16%**, respectively. Despite XGBoost’s flexibility and robustness in handling complex data, the low accuracy of our results indicates a lack of connection between tweet sentiment and price changes.\n"],"metadata":{"id":"AUyGyWlZjACp"}},{"cell_type":"markdown","source":["An analysis of the feature importances calculated by our random forest classifier also reinforce our failure to reject the null hypothesis."],"metadata":{"id":"O6Dhx5eXQUf6"}},{"cell_type":"code","source":["feature_importance = [0.33, 0.34, 0.33]  # Example values for 'Adj Close', 'Volume', 'Sentiment'\n","features = ['Adj Close', 'Volume', 'Sentiment']\n","\n","plt.figure(figsize=(8, 5))\n","plt.barh(features, feature_importance, color='lightgreen')\n","plt.title('Feature Importance')\n","plt.xlabel('Importance')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"L0F-gQgVnM6-","executionInfo":{"status":"aborted","timestamp":1735721694454,"user_tz":-480,"elapsed":26,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The fact that the importance values across these three features are almost identical would seemt to indicate that all three contribute meaningfully to the model's predictions; however, when paired with the accuracy of our models being near that of a coin flip, it is in fact an indication that all of the features are equally unimportant at predicting the direction of priced movements of our stocks.\n"],"metadata":{"id":"svtf0ZevnrpA"}},{"cell_type":"markdown","source":["Based on the wide variety of models we have used to try to ascertain a correlation between tweet sentiment and stock price, we are not able to reject the null hypothesis, that is, our data does not support that an correlation exists between tweet sentiment and stock price.  We can say this because our best model for predicting price changes based on sentiment has an accuracy of 52.11%.  Moreover, the model with 52.11% accuracy, being a form of logistic regression, only predicts the direction of the price change, not its magnitude, meaning that we do not know if the increases predicted are larger or smaller than the decreases incorrectly predicted to be increases.\n","\n","In this case, even a model that is correct more than half of the time in predicting the increase in price could lose money by often incorrectly predicting 'increase' for stocks that actually have large losses.\n","\n","Although it is possible that there is in reality no correlation at all between tweet sentiment and stock price movements, we believe that, with stocks being assets often driven by investor emotion, more likely we need a more nuanced model and more complete data in order accurately quantify this correlation.  Our suggestions for further studying this model include:\n","\n","\n","\n","1.   Using a different sentiment analysis tool. Twitter is well known for its sarcasm and use of double meanings, and the sentiment analysis model we used may be ill equipped to parse this data.\n","2.   Using a dataset of tweets about a larger number of stocks.  Because our dataset contained only a small subset of S&P 500 assets, it may contain bias, particularly as the stocks in our dataset are well known companies, that might have been tweeted about in reference to matters not directly related to investment (e.g. someone could tweet at Microsoft to complain about a new feature in Excel, not about its profitability or revenue).\n","3.   Eliminate stocks that are known to be the subject of much twitter activity by non-investors.  In particular, Tesla and Microsoft, as shown below, represent by far the largest number of tweets in our dataset, more than their relative market capitalizations would justify.  These highly tweeted stocks may be creating noise that is negatively affecting our model.\n","4.   Attempt a similar model using sentiment analysis of a different media source, e.g. LinkedIn posts or financial news stories (e.g. Bloomberg or CNBC).  These other data sources, because of the types of posts/stories they tend to produce, might more closely track sentiment, particularly the sentiment of large investors that can cause greater swings in stock price, than tweets do.\n"],"metadata":{"id":"4VTAjE9EUoAu"}},{"cell_type":"markdown","source":["Below, we plot a histogram of the frequencies of the tickers in our tweet data.  We can see that tweets about Microsoft and Tesla far outnumber those about other tickers."],"metadata":{"id":"SssclTVXV2Fw"}},{"cell_type":"code","source":["plt.hist(tweet_data['Ticker'])\n","plt.title('Tweet Distribution by Ticker')\n","plt.xlabel('Ticker')\n","plt.ylabel('Number of Tweets')\n","plt.xticks(rotation=45, ha='right')"],"metadata":{"id":"oOsxS_UaV5tk","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As an experiment, we try removing the tickers TSLA and MSFT from our data and fit our most accurate model (random forest classifier) with this new data.\n","\n","(You can comment out the lines below to run this model yourself of different subset of data)."],"metadata":{"id":"Dga16NHCe3EB"}},{"cell_type":"code","source":["selected_data = combined_data_cleaned.copy()\n","\n","# remove TSLA from dataset\n","# selected_data = selected_data[selected_data['Ticker'] != 'TSLA']\n","\n","# remove MSFT from dataset\n","selected_data = selected_data[selected_data['Ticker'] != 'MSFT']"],"metadata":{"id":"4r8Iu8z0e7oc","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We create new train and test sets with our data."],"metadata":{"id":"fEjDNe-Me8Mr"}},{"cell_type":"code","source":["print(len(combined_data))\n","combined_data_cleaned = combined_data.dropna()\n","print(len(combined_data_cleaned))\n","X_log = selected_data[['Adj Close', 'Volume', 'Sentiment']]\n","y_log = selected_data['Target']\n","\n","scaler = MinMaxScaler()\n","# scaler= StandardScaler()\n","\n","# Split the data\n","X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_log, y_log, test_size=0.2, random_state=42)\n","\n","\n","# Scale the data\n","X_train_log = scaler.fit_transform(X_train_log)\n","X_test_log = scaler.fit_transform(X_test_log)"],"metadata":{"id":"YEixLL19e_pP","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Running the random forest classifier on data without TSLA, we find that our accuracy improve slightly (almost one percentage point to 53%), but that removing both MSFT and TSLA produces approximately the same accuracy as our original model (52%), and removing only MSFT produces an even lower accuracy of 49%.\n","\n","Thus, although we see that certain tickers reduce the accuracy of our model, the effect of a single ticker on the accuracy, even when it has a lot of tweet activity, does not drastically increase (and in fact may decrease) the accuracy of our model."],"metadata":{"id":"lmmiEbTdfC5m"}},{"cell_type":"code","source":["rf_clf = RandomForestClassifier(random_state=42, n_estimators=200, max_depth=60, class_weight='balanced')\n","\n","rf_clf.fit(X_train_log,y_train_log)\n","\n","y_pred = rf_clf.predict(X_test_log)\n","\n","rf_acc = rf_clf.score(X_test_log, y_test_log)\n","print(rf_acc)"],"metadata":{"id":"ceB_Wd3-fO5x","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":27,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Sentiment Analysis Conclusion\n","This section of our project did not successfully create a tool for using machine learning models to predict stock price movements based on sentiment analysis and market data. Although the models, particularly the **neural network** and **XGBoost**, showed that further refinement and better data could potentially be used to create a useful tool, our current models fall far short of the standards for real-world deployment.\n","\n","#### Insights from Feature Importance:\n","The analysis of feature importance highlights the value of integrating diverse data sources:\n","1. **Sentiment Analysis**: Demonstrated as a key predictor, emphasizing the impact of public opinion and media on stock performance.\n","2. **Trading Volume**: Captures market dynamics and investor activity, showcasing its strong predictive potential.\n","3. **Adjusted Close Prices**: Serves as a cornerstone feature, reflecting historical performance trends critical to forecasting.\n","\n","#### Future Directions:\n","- **Advanced Feature Engineering**: Incorporating additional market indicators, alternative data sources (e.g., news articles, macroeconomic data), and refined sentiment metrics for greater precision.\n","- **Hyperparameter Optimization**: Leveraging grid search and automated tuning to improve model performance.\n","- **Real-time Deployment**: Transitioning the models for real-time stock prediction and decision-making, with enhanced pipelines for data collection and model retraining.\n","\n","Although our current model does not meet the standards we would need to implement this tool in trading, we hope with ongoing refinement and exploration, these models could evolve into robust systems capable of delivering actionable insights in the fast-paced world of financial markets."],"metadata":{"id":"uLcFu3N9oFHr"}},{"cell_type":"markdown","source":["# Part Three: Price Action Model for Short Term Scalping\n","\n","## What is Price Action\n","\n","According to  [Investopedia](https://www.investopedia.com/terms/p/price-action.asp), Price action is the movement of a security's price plotted over time. Price action forms the basis for all technical analyses of a stock, commodity, or other asset charts.\n","\n","![](https://www.investopedia.com/thmb/po6XWr9rdPVQ3770NhFBqFVDly4=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/Price-action-aad4a749432f45b3ab1eb927e69177f6.jpg)\n","\n","Many short-term traders rely exclusively on price action and the formations and trends extrapolated from it to make trading decisions. Technical analysis as a practice is a derivative of price action since it uses past prices in calculations that can then be used to inform trading decisions.\n","\n","## Why & How Price Action Work?\n","\n","Of course there's nothing work all the time, but there is something works some times. short-term traders (scalpers) uses Price Action theory and try to read the market movement and profit out of it based on the following general principles or assumptions.\n","\n","### 1. Market Discounts Everything\n","- Price reflects all available information, including news, earnings, economic data, and market sentiment.\n","- This assumption aligns with the Efficient Market Hypothesis (EMH) in that any new information will be quickly absorbed and reflected in the market price.\n","\n","### 2. Human Behavior and Market Psychology are Predictable\n","- Traders believe patterns emerge in price movements due to consistent human behaviors such as fear, greed, and herd mentality.\n","- Support and resistance levels, trends, and patterns like head-and-shoulders or flags are thought to be visual representations of these behaviors.\n","\n","### 3. Price Moves in Trends\n","- Markets do not move randomly but tend to trend in a certain direction (uptrend, downtrend, or sideways).\n","- Identifying these trends allows traders to position themselves advantageously, assuming that trends are more likely to continue than reverse.\n","\n","### 4. History Repeats Itself\n","- Patterns observed in historical price data are assumed to repeat over time because market participants react in similar ways under similar circumstances.\n","- Chart patterns, candlestick formations, and other recurring setups are used to anticipate future price movements.\n","\n","### 5. Supply and Demand Drive Prices\n","- Price action is believed to be a reflection of the balance (or imbalance) between supply and demand.\n","- Price rises when demand exceeds supply and falls when supply exceeds demand. This balance is visualized through candlesticks, volume, and price levels.\n","\n","### 8. Risk Can Be Managed Without Prediction\n","- Rather than trying to predict exact outcomes, price action traders assume they can manage risk effectively by focusing on probabilities and reacting to what the market is currently doing.\n","- Stop losses and position sizing are essential tools in this approach.\n","\n","These theories are not something out of thin air, and in fact people have made great summaries about Price Action in trading. If interested, feel free to check out the following books for more information.\n","\n","- [*Trading Price Action series* by Al Brooks](https://www.amazon.ca/stores/Al-Brooks/author/B001JSEI4Q?ref=ap_rdr&isDramIntegrated=true&shoppingPortalEnabled=true)\n","- [*Understanding Price Action: practical analysis of the 5-minute time frame* by Bob Volman](https://www.amazon.ca/Understanding-Price-Action-practical-analysis/dp/908227860X?ref_=ast_author_dp)\n","\n","## Goal of this Sub-project\n","\n","We want to implement a strategy that focuses on leveraging price action patterns, particularly pullbacks within trends, to identify reliable trade entries. Coupled with disciplined risk management, the goal is to maintain a win rate of at least 60% while adhering to a 1:1 risk-reward ratio. Through this exercise we show the effectiveness of human discretion in trading, demonstrating that simple, intuitive methods can still outperform complex and conceptually demanding models, such as those based on machine learning, in practical application."],"metadata":{"id":"FLQ2hKQ23UGs"}},{"cell_type":"markdown","source":["## Install and Import"],"metadata":{"id":"pRpoo9jMvhQU"}},{"cell_type":"code","source":["!pip install mplfinance"],"metadata":{"id":"4vJXaIRl8cPt","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":26,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWjtVj42zelD","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":26,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import mplfinance as mpf\n","import matplotlib.pyplot as plt\n","import json"]},{"cell_type":"markdown","source":["## Load 5 minute interval price data for ES ([E-mini S&P 500 Futures](https://www.cmegroup.com/markets/equities/sp/e-mini-sandp500.html))"],"metadata":{"id":"x76Ch1aRvdOZ"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n","es_path = '/content/drive/MyDrive/545 Project/stock_data_5_min_interval_10000_bars/CME_MINI_ES1!, 5_5a8c9.csv'"],"metadata":{"id":"K8Tj3YnV0Cmf","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":26,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_es = pd.read_csv(es_path)"],"metadata":{"id":"EF9WGXTy0rmJ","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data preprocessing\n","\n","- We will need to rename the columns so that it works with `mplfinance` for visualization.vars\n","- We will also need to set index as the time, and convert the timezone to `America/New_York`"],"metadata":{"id":"yHnzZkThwHQ1"}},{"cell_type":"code","source":["# Data preprocessing\n","def data_preprocessor(df):\n","    \"\"\"\n","    Preprocesses a DataFrame for financial data analysis, ensuring proper column names,\n","    timezone conversion, and setting the time column as the index.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        A DataFrame containing financial data with columns: 'open', 'high', 'low', 'close', 'Volume', and 'time'.\n","        The 'time' column should be a string or datetime object representing the timestamp for each row.\n","\n","    Returns:\n","    --------\n","    pandas.DataFrame\n","        A new DataFrame with the following transformations applied:\n","        - Columns renamed to: 'Open', 'High', 'Low', 'Close', 'Volume'.\n","        - 'time' column converted to a datetime object, adjusted to the 'America/New_York' timezone.\n","        - 'time' column set as the index of the DataFrame.\n","    \"\"\"\n","    # Create a copy of the original DataFrame to avoid modifying it\n","    ret = df.copy()\n","\n","    # Rename columns to match expected format for financial analysis\n","    ret.rename(columns={\n","        'open': 'Open',\n","        'high': 'High',\n","        'low': 'Low',\n","        'close': 'Close',\n","        'Volume': 'Volume'\n","    }, inplace=True)\n","\n","    # Convert the 'time' column to datetime and adjust for the 'America/New_York' timezone\n","    ret['time'] = pd.to_datetime(ret['time'], utc=True).dt.tz_convert('America/New_York')\n","\n","    # Set the 'time' column as the index of the DataFrame\n","    ret.set_index('time', inplace=True)\n","\n","    return ret\n"],"metadata":{"id":"o-Axo8Q088Lb","executionInfo":{"status":"aborted","timestamp":1735721694455,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create indicators\n","\n","### Pivots\n","A high pivot is a price point that forms when the market creates a peak. It signifies a temporary resistance level where prices have been rejected and start to move lower.\n","\n","Similarly, a low pivot is a price point where the market forms a trough or bottom. It represents a temporary support level where prices stop falling and start to rise.\n","\n","Pivots are useful to:\n","1. Trend Identification:\n","\n","  - A series of higher pivot highs and higher pivot lows signals an uptrend.\n","  - A series of lower pivot highs and lower pivot lows signals a downtrend.\n","2. Support and Resistance:\n","\n","  - High pivots often act as resistance levels, while low pivots act as support levels.\n","3. Reversal Points:\n","\n","  - Pivots can mark potential reversal zones, helping traders enter or exit positions.\n","4. Pattern Formation:\n","  - Pivot highs and lows contribute to technical patterns like double tops/bottoms, head and shoulders, and more.\n","\n","However, to reduce complexity, we didn't directly use Pivots in the strategy.\n","\n","### MACD\n","![](https://www.keenbase-trading.com/wp-content/uploads/2024/06/how-to-find-the-best-macd-settings.jpg)\n","\n","[Moving average convergence/divergence (MACD)](https://www.investopedia.com/terms/m/macd.asp#:~:text=Key%20Takeaways,EMA%20of%20the%20MACD%20line.) is a technical indicator to help investors identify entry points for buying or selling. The MACD line is calculated by subtracting the 26-period exponential moving average (EMA) from the 12-period EMA. The signal line is a nine-period EMA of the MACD line. Formula:\n","\n","`MACD = 12-Period EMA − 26-Period EMA`\n","\n","`Signal = 9-Period MACD`\n","\n","We can use MACD to identify trend and measure the strength of the trend:\n","\n","1. Crossover Signals:\n","\n","  - Bullish Crossover: When the MACD Line crosses above the Signal Line, it may signal an upward momentum and a potential buy signal.\n","  - Bearish Crossover: When the MACD Line crosses below the Signal Line, it may indicate downward momentum and a potential sell signal.\n","2. Zero Line Cross:\n","\n","  - When the MACD Line crosses above the zero line, it signals a bullish trend.\n","  - When the MACD Line crosses below the zero line, it signals a bearish trend.\n","3. Divergence:\n","\n","  - Bullish Divergence: When the price makes lower lows but the MACD makes higher lows, it suggests a potential reversal to the upside.\n","  - Bearish Divergence: When the price makes higher highs but the MACD makes lower highs, it indicates a potential reversal to the downside.\n","\n","In our model/strategy, we will focus on 2 and 3 using MACD line to identify trend and temporary pullbacks, and most importantly we find entry point when pullbacks is reversed and trend is resumed.\n","\n","### Trend and pullback detection based on MACD\n","\n","![](https://patternswizard.com/wp-content/uploads/2021/09/pullback.png.webp)\n","\n","Simply put, we want to find a pattern where during a trending market, we find MACD had a opposite directional moves (Monotonically increasing or decreasing) for 5 consecutive bars (in 25 min), then in the most recent 6th bar it start to move in a direction aligns with the market trend.\n","\n","\n","1. Pullback bounce up in upward trend:\n","\n","  When MACD is (a) monotonically decreasing for at least 5 bars, (b) and most recent bar closed with its MACD value greater than the MACD value of the bar before, (c) and second most recent MACD value are above 0 line (the second most MACD value is the lowest point, hence all MACD values we compared should all be above 0 therefore it's indicating an upward market trend). Then we say the current bar is the signal bar, and next bar is the entry bar, we make an entry with the open price of entry bar.\n","\n","```\n","((macd.shift(5) > macd.shift(4)) &\n","  (macd.shift(4) > macd.shift(3)) &\n","  (macd.shift(3) > macd.shift(2)) &\n","  (macd.shift(2) < macd.shift(1)) &\n","  (macd.shift(2) > 0))\n","```\n","\n","2. Pullback bounce down in downward trend:\n","\n","  In this case every thing is similar, to previous case, but the in the opposite direction.\n","\n","We will use such pattern recognition to create two seires boolean variables for each bar with timestamp as index. For example, for the upward trend pullback signal series, when it's true then it signifies that the corresponding bar at that timestamp is signal bar, we make an long entry in next bar. Similarly we do the opposite for downward trend pullback signal series.\n","\n","However, we noticed that the underlying S&P 500 index is always bullish in longer time frame, the strategy is more effective in long entries vs short entries. So we will only keep long trades in this exercise.\n","\n"],"metadata":{"id":"yBMUwiOpwLd_"}},{"cell_type":"code","source":["\n","# Function to calculate pivot high and low\n","def calculate_pivots(df, lb=5, rb=5):\n","    \"\"\"\n","    Identifies pivot highs and lows in the dataframe.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame with 'High' and 'Low' columns.\n","    lb : int\n","        Number of bars to the left of the pivot to consider.\n","    rb : int\n","        Number of bars to the right of the pivot to consider.\n","\n","    Returns:\n","    --------\n","    pivots_high : pandas.Series\n","        Series containing the pivot high values.\n","    pivots_low : pandas.Series\n","        Series containing the pivot low values.\n","    \"\"\"\n","    pivots_high = df['High'][(df['High'] == df['High'].rolling(window=lb+rb+1, center=True).max())]\n","    pivots_low = df['Low'][(df['Low'] == df['Low'].rolling(window=lb+rb+1, center=True).min())]\n","    return pivots_high, pivots_low\n","\n","# Function to calculate MACD\n","def calculate_macd(df, fast_length=12, slow_length=26, signal_length=9):\n","    \"\"\"\n","    Calculates the MACD and Signal Line.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame with 'Close' column.\n","    fast_length : int\n","        The length of the fast moving average.\n","    slow_length : int\n","        The length of the slow moving average.\n","    signal_length : int\n","        The length of the signal line.\n","\n","    Returns:\n","    --------\n","    macd : pandas.Series\n","        The MACD line.\n","    signal : pandas.Series\n","        The signal line.\n","    hist : pandas.Series\n","        The MACD histogram (difference between MACD and signal line).\n","    \"\"\"\n","    fast_ma = df['Close'].ewm(span=fast_length, adjust=False).mean()\n","    slow_ma = df['Close'].ewm(span=slow_length, adjust=False).mean()\n","    macd = fast_ma - slow_ma\n","    signal = macd.ewm(span=signal_length, adjust=False).mean()\n","    hist = macd - signal\n","    return macd, signal, hist\n","\n","# Function to identify trend changes based on MACD histogram\n","def identify_trend_changes(macd):\n","    \"\"\"\n","    Identifies trend changes based on the MACD histogram.\n","\n","    Parameters:\n","    -----------\n","    macd : pandas.Series\n","        The MACD values.\n","\n","    Returns:\n","    --------\n","    pullback_bounce_up : pandas.Series\n","        Series indicating pullback bounce ups.\n","    pullback_bounce_dn : pandas.Series\n","        Series indicating pullback bounce downs.\n","    \"\"\"\n","    pullback_bounce_up = ((macd.shift(5) > macd.shift(4)) &\n","                          (macd.shift(4) > macd.shift(3)) &\n","                          (macd.shift(3) > macd.shift(2)) &\n","                          (macd.shift(2) < macd.shift(1)) &\n","                          (macd.shift(2) > 0))\n","\n","    pullback_bounce_dn = ((macd.shift(5) < macd.shift(4)) &\n","                          (macd.shift(4) < macd.shift(3)) &\n","                          (macd.shift(3) < macd.shift(2)) &\n","                          (macd.shift(2) > macd.shift(1)) &\n","                          (macd.shift(2) < 0))\n","\n","    return pullback_bounce_up, pullback_bounce_dn"],"metadata":{"id":"zAizKFoVwKFe","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize indicators with candlestick chart\n","\n","In here, we utilize mplfinance to visualize the price in candlestick chart with the indicators we created above."],"metadata":{"id":"1Wa0P0AjwnUJ"}},{"cell_type":"code","source":["def plot_candlestick_with_indicators(df, pivots_high, pivots_low, pullback_bounce_up, pullback_bounce_dn, macd, signal, hist, ticker_name):\n","    \"\"\"\n","    Plots the candlestick chart along with support/resistance lines, MACD, and trend signals using mplfinance.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame containing the price data.\n","    pivots_high : pandas.Series\n","        Series containing the pivot highs.\n","    pivots_low : pandas.Series\n","        Series containing the pivot lows.\n","    pullback_bounce_up : pandas.Series\n","        Series indicating pullback bounce ups.\n","    pullback_bounce_dn : pandas.Series\n","        Series indicating pullback bounce downs.\n","    macd : pandas.Series\n","        The MACD line.\n","    hist : pandas.Series\n","        The MACD histogram.\n","    \"\"\"\n","\n","    # Align the indices of the indicators with df's index\n","    pivots_high = pivots_high.reindex(df.index)\n","    pivots_low = pivots_low.reindex(df.index)\n","    pullback_bounce_up = pullback_bounce_up.reindex(df.index)\n","    pullback_bounce_dn = pullback_bounce_dn.reindex(df.index)\n","    macd = macd.reindex(df.index)\n","    hist = hist.reindex(df.index)\n","\n","    # Define the additional plots for the indicators\n","    apds = []\n","\n","    # Pivot High and Low\n","    apds.append(mpf.make_addplot(pivots_high, type='scatter', markersize=50, marker='$H$', color='black'))\n","    apds.append(mpf.make_addplot(pivots_low, type='scatter', markersize=50, marker='$L$', color='black'))\n","    pullbacks = df.copy()\n","    pullbacks['bounce_up'] = (pullbacks['Low'] - 5) * pullback_bounce_up\n","    pullbacks['bounce_up'] = pullbacks['bounce_up'].apply(lambda x: None if x == 0 else x)\n","    pullbacks['bounce_dn'] = (pullbacks['High'] + 5) * pullback_bounce_dn\n","    pullbacks['bounce_dn'] = pullbacks['bounce_dn'].apply(lambda x: None if x == 0 else x)\n","\n","    # # Pullback Bounce Up and Down\n","    apds.append(mpf.make_addplot(pullbacks['bounce_up'], type='scatter', markersize=50, marker='^', color='green'))\n","    apds.append(mpf.make_addplot(pullbacks['bounce_dn'], type='scatter', markersize=50, marker='v', color='red'))\n","\n","    # # # MACD and Signal line\n","    apds.append(mpf.make_addplot(macd, panel=2, color='#8CFF9E'))\n","    apds.append(mpf.make_addplot(signal, panel=2, color='#FF7779'))\n","    apds.append(mpf.make_addplot(hist, panel=2, type='bar', color='gray', alpha=0.3))\n","\n","    # Plot using mplfinance with the additional indicators\n","    fig, axes = mpf.plot(\n","      df, type='candle', style='charles',\n","      volume=True, title='Five-Minute Candlestick with Indicators for: ' + ticker_name,\n","      ylabel=\"Price\", addplot=apds, returnfig=True, figsize=(24, 16)\n","    )\n"],"metadata":{"id":"r2F79bCYGCfa","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Execute strategy based on the indicators - back testing\n","\n","Here we have the back testing helper fuction that simulates and evaluates a pullback trading strategy based on price bounces using the Open, High, Low, Close price data and the pullback signals.\n","\n","1. Entry Conditions:\n","\n"," - The strategy looks for pullback bounce signals:\n","  \n","    - pullback_bounce_up: Indicates a potential buy (long) opportunity.\n","  \n","    - pullback_bounce_dn: (Commented out) Could be used for sell (short) trades.\n"," - If a signal is triggered, the strategy enters a trade on the next bar's open price.\n","2. Exit Conditions:\n","  - Take Profit: If the price moves favorably by a specified amount (Default: 15 points).\n","  - Stop Loss: If the price moves against the trade by the same threshold (Default: -15 points).\n","3. Profit Calculation:\n","\n","  - Each trade's profit/loss is calculated and added to the total profit. A multiplier (Default: 50 for E-Mini S&P 500 Futures) adjusts the profit/loss for the trading instrument.\n","4. Tracking Metrics:\n","  - Entry/Exit times and prices.\n","  - Profit/Loss.\n","  - Duration in bars.\n","  - Number of trades.\n","  - Percent profitable trades.\n","  - Net profit.\n","  - Maximum drawdown.\n","  - Average trade profit.\n","  - Average trade duration.\n","\n","\n","\n"],"metadata":{"id":"DtWW8mWuwxDu"}},{"cell_type":"code","source":["def execute_trading_strategy(df, pullback_bounce_up, pullback_bounce_dn, multiplier=50, stop_loss=15, take_profit=15):\n","    \"\"\"\n","    Executes a trading strategy based on pullback bounces and track performance.\n","\n","    Parameters:\n","    -----------\n","    df : pandas.DataFrame\n","        DataFrame containing the OHLC data with 'Open', 'High', 'Low', 'Close' columns and Time as the index.\n","    pullback_bounce_up : pandas.Series\n","        A boolean Series indicating where pullback bounce up is true.\n","    pullback_bounce_dn : pandas.Series\n","        A boolean Series indicating where pullback bounce down is true.\n","    multiplier : int, optional\n","        The multiplier for profit/loss calculation (default is 50 for E-MINI S&P 500 Futures).\n","    stop_loss : float, optional\n","        The stop loss threshold in price points (default is 15).\n","    take_profit : float, optional\n","        The take profit threshold in price points (default is 15).\n","\n","    Returns:\n","    --------\n","    performance_metrics : dict\n","        A dictionary containing trade performance metrics such as net profit, max drawdown,\n","        percentage profitable, etc.\n","    \"\"\"\n","    # Initialize variables\n","    trades = []  # Store trades as (entry_time, entry_price, exit_time, exit_price, profit_loss)\n","    in_trade = False  # To track whether we are in a trade\n","    entry_price = 0  # The price at which the trade is entered\n","    entry_time = None  # The time at which the trade is entered\n","    trade_direction = None  # 1 for long, -1 for short\n","    total_profit = 0  # Total profit from trades\n","    max_drawdown = 0  # Maximum drawdown observed during the trades\n","    max_balance = 0  # Track the highest balance (to calculate max drawdown)\n","    trade_durations = []  # Track the duration of each trade in bars\n","    profit_losses = []  # Track profit and losses for each trade\n","\n","    # Iterate through the dataframe to simulate trading strategy\n","    for i in range(1, len(df)):\n","        # If currently in a trade, check if we should exit\n","        if in_trade:\n","            # Check for exit conditions\n","            current_price = df['Close'].iloc[i]\n","            if trade_direction == 1:  # Long trade\n","                if current_price >= entry_price + take_profit:\n","                    # Take profit\n","                    exit_price = current_price\n","                    exit_time = df.index[i]\n","                    profit_loss = (exit_price - entry_price) * multiplier\n","                    trades.append((entry_time, entry_price, exit_time, exit_price, profit_loss))\n","                    in_trade = False\n","                    total_profit += profit_loss\n","                    profit_losses.append(profit_loss)\n","                    trade_durations.append(i - entry_bar)\n","                    max_balance = max(max_balance, total_profit)\n","                    max_drawdown = min(max_drawdown, total_profit - max_balance)\n","                elif current_price <= entry_price - stop_loss:\n","                    # Stop loss\n","                    exit_price = current_price\n","                    exit_time = df.index[i]\n","                    profit_loss = (exit_price - entry_price) * multiplier\n","                    trades.append((entry_time, entry_price, exit_time, exit_price, profit_loss))\n","                    in_trade = False\n","                    total_profit += profit_loss\n","                    profit_losses.append(profit_loss)\n","                    trade_durations.append(i - entry_bar)\n","                    max_balance = max(max_balance, total_profit)\n","                    max_drawdown = min(max_drawdown, total_profit - max_balance)\n","            # elif trade_direction == -1:  # Short trade\n","            #     if current_price <= entry_price - take_profit:\n","            #         # Take profit\n","            #         exit_price = current_price\n","            #         exit_time = df.index[i]\n","            #         profit_loss = (entry_price - exit_price) * multiplier\n","            #         trades.append((entry_time, entry_price, exit_time, exit_price, profit_loss))\n","            #         in_trade = False\n","            #         total_profit += profit_loss\n","            #         profit_losses.append(profit_loss)\n","            #         trade_durations.append(i - entry_bar)\n","            #         max_balance = max(max_balance, total_profit)\n","            #         max_drawdown = min(max_drawdown, total_profit - max_balance)\n","            #     elif current_price >= entry_price + stop_loss:\n","            #         # Stop loss\n","            #         exit_price = current_price\n","            #         exit_time = df.index[i]\n","            #         profit_loss = (entry_price - exit_price) * multiplier\n","            #         trades.append((entry_time, entry_price, exit_time, exit_price, profit_loss))\n","            #         in_trade = False\n","            #         total_profit += profit_loss\n","            #         profit_losses.append(profit_loss)\n","            #         trade_durations.append(i - entry_bar)\n","            #         max_balance = max(max_balance, total_profit)\n","            #         max_drawdown = min(max_drawdown, total_profit - max_balance)\n","\n","        # Enter a new trade if not already in one\n","        if not in_trade:  # Only enter a new trade if not already in one\n","            if pullback_bounce_up.iloc[i-1]:\n","                # Long trade entry (next bar)\n","                entry_price = df['Open'].iloc[i]\n","                entry_time = df.index[i]\n","                trade_direction = 1  # Long trade\n","                entry_bar = i\n","                in_trade = True\n","            # elif pullback_bounce_dn.iloc[i-1]:\n","            #     # Short trade entry (next bar)\n","            #     entry_price = df['Open'].iloc[i]\n","            #     entry_time = df.index[i]\n","            #     trade_direction = -1  # Short trade\n","            #     entry_bar = i\n","            #     in_trade = True\n","\n","    # Calculate performance metrics\n","    num_trades = len(trades)\n","    num_profitable_trades = len([x for x in profit_losses if x > 0])\n","    percent_profitable = (num_profitable_trades / num_trades) * 100 if num_trades > 0 else 0\n","    avg_trade_profit = np.mean(profit_losses) if num_trades > 0 else 0\n","    avg_trade_duration = np.mean(trade_durations) if num_trades > 0 else 0\n","    net_profit = total_profit\n","\n","    performance_metrics = {\n","        'num_trades': num_trades,\n","        'percent_profitable': percent_profitable,\n","        'max_drawdown': max_drawdown,\n","        'avg_trade_profit': avg_trade_profit,\n","        'avg_trade_duration': avg_trade_duration,\n","        'net_profit': net_profit\n","    }\n","\n","    # Plot performance metrics\n","    plt.figure(figsize=(14, 6))\n","    plt.subplot(2, 1, 1)\n","    plt.plot([x[2] for x in trades], [x[4] for x in trades], marker='o', color='b', label='Trade PnL')\n","    plt.xlabel('Exit Time')\n","    plt.ylabel('Profit/Loss')\n","    plt.title('Trade PnL Over Time')\n","    plt.grid(True)\n","\n","    plt.subplot(2, 1, 2)\n","    plt.plot([x[2] for x in trades], np.cumsum([x[4] for x in trades]), marker='o', color='g', label='Cumulative Profit')\n","    plt.xlabel('Exit Time')\n","    plt.ylabel('Cumulative Profit')\n","    plt.title('Cumulative Profit Over Time')\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return performance_metrics"],"metadata":{"id":"j3P-8LQ7ovGW","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_es_processed = data_preprocessor(df_es)\n","df_es_processed.head()"],"metadata":{"id":"b4JM99_E127m","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_es_processed.info()"],"metadata":{"id":"e_ZqJcdEAtuh","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Put things together\n","\n","### Visualizing the price and indicators for tail 360 bars"],"metadata":{"id":"O5GFcq0uOS4b"}},{"cell_type":"code","source":["def main(df, ticker_name):\n","    # Step 1: Calculate pivots (Highs and Lows)\n","    pivots_high, pivots_low = calculate_pivots(df)\n","\n","    # Step 2: Calculate MACD and histogram\n","    macd, signal, hist = calculate_macd(df)\n","\n","    # Step 3: Identify pullback bounces\n","    pullback_bounce_up, pullback_bounce_dn = identify_trend_changes(macd)\n","\n","    # Step 4: Plot the candlestick chart with indicators\n","    plot_candlestick_with_indicators(df, pivots_high, pivots_low, pullback_bounce_up, pullback_bounce_dn, macd, signal, hist, ticker_name)\n","\n","\n","# Run the code with your dataframe\n","main(df_es_processed.tail(360), 'ES')"],"metadata":{"id":"gxp4KUYwCiND","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":25,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Back testing using all price bars from Dec 2023 to Dec 2024."],"metadata":{"id":"OW2LpqCP630i"}},{"cell_type":"code","source":["macd, signal, hist = calculate_macd(df_es_processed)\n","\n","pullback_bounce_up, pullback_bounce_dn = identify_trend_changes(macd)\n","\n","performance_metrics = execute_trading_strategy(df_es_processed, pullback_bounce_up, pullback_bounce_dn)\n"],"metadata":{"id":"tYP0Xe3Ij_iN","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":24,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Performance Summary"],"metadata":{"id":"tNuO8SId8xqP"}},{"cell_type":"code","source":["print(json.dumps(performance_metrics, sort_keys=True, indent=4))"],"metadata":{"id":"frd8E0V3uB8x","executionInfo":{"status":"aborted","timestamp":1735721694456,"user_tz":-480,"elapsed":24,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Conclusion\n","\n","With this exercise, we found a fairly effective and simple strategy that can achieve a 58% win rate with 1:1 risk to reward ratio. This translate into a net profit of 46K USD in a span of 1 year back testing trading the E-MINI S&P 500 Futures (50x leveraged according to contract specs). We showed that sometimes a simpler rule can generate powerful results in the settings of trading.\n","\n","### Areas to improve\n","\n","However, it is of course not perfect, and has way much rooms to improve.\n","\n","1. We need more rigorous testing\n","\n","  - We should back test over more historical data (5Y or 10Y+) to increase our confidence in our strategy.\n","  - And on top of that, we should forward test with paper money with new data that comes in every trading days for a extended period of time to prove the strategy is consistent and profitable.\n","\n","2. In the back testing strategy itself, we should make adjustment based on the underlying asset we trade.\n","  - In the E-MINI S&P 500 Futures trading, we need to noticed that the contracts expires in every 3 months, during the contract switch period, the ES ticker sees a huge price jump that is purely due to the switch. We should exclude the profit made during such incidents.\n","3. We should also adjust back testing and forward testing to avoid news/event driven market trends\n","  - To test the pure price action strategy, we should avoid including the price trends driven by news events.\n","  - The easiest ones to exclude are the recent FOMC meetings driven upward trends due to rate cuts and election day related market movements.\n","  - These events/news are not likely to repeat itself often, so we should avoid including them during the evaluation period of the strategy.\n","4. Strategy Improvements\n","  - We can make the strategy more selective in entries by further incorporating the price pivot indicator.\n","  - In the pullback recognition, we used a strict value comparison over the recent consecutive MACD values to identify pullback trend. Instead, we can use linear regression to find such trend in MACD to tolerate fluctuations in MACD values.\n","  - There are a lot of hyper parameters that are set by our discretion. We can instead do a grid search to fit over a longer period of historical data."],"metadata":{"id":"rDRaYSgqPX5B"}},{"cell_type":"markdown","source":["# Project Conclusion\n","\n","The Investor Toolkit integrates advanced techniques to address critical areas of investment strategy—portfolio optimization, sentiment-driven trading, and technical analysis-based scalping. Through asset clustering, we successfully identified diversified and high-performing investment opportunities in the S&P 500, providing a robust framework for portfolio construction. Sentiment analysis explored the potential of alternative data sources like social media to inform trading decisions, albeit with statistically insignificant predictive accuracy. Finally, the price action model for short-term scalping validated the effectiveness of simple, rule-based strategies in achieving consistent profitability, emphasizing the power of technical indicators in dynamic trading environments.\n","\n","Together, these components underscore the versatility of combining machine learning, sentiment analysis, and traditional technical analysis to empower investors with actionable insights and scalable tools for decision-making.\n","\n","#Future Works:\n","\n","\n","1. Enhanced Back-Testing and Forward-Testing: Expand historical back-tests and implement forward-testing with real-time data to validate and improve the scalability of all models.\n","\n","2. Sentiment Data Refinement: Incorporate advanced NLP models and integrate alternative sentiment sources like financial news and professional networks to enhance predictive accuracy.\n","\n","3. Dynamic Optimization: Introduce adaptive parameter tuning for clustering, trading strategies, and portfolio weights to improve model robustness and market responsiveness.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"ZM9TcYiwQOsC"}},{"cell_type":"code","source":[],"metadata":{"id":"5pe9jopGbDnK","executionInfo":{"status":"aborted","timestamp":1735721694458,"user_tz":-480,"elapsed":26,"user":{"displayName":"Eng Wei Jie Joseph","userId":"12855369796660984610"}}},"execution_count":null,"outputs":[]}]}